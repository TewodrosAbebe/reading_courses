@inproceedings{Baroni:2012aa,
  author =        {Baroni, Marco and Bernardi, Raffaella and
                   Do, Ngoc-Quynh and Shan, Chung-chieh},
  booktitle =     {Proceedings of the 13th Conference of the European
                   Chapter of the Association for Computational
                   Linguistics},
  organization =  {Association for Computational Linguistics},
  pages =         {23--32},
  title =         {Entailment above the word level in distributional
                   semantics},
  year =          {2012},
}

@inproceedings{Bhaskar:2017aa,
  author =        {Bhaskar, Sai Abishek and K{\"o}per, Maximilian and
                   Im Walde, Sabine Schulte and Frassinelli, Diego},
  booktitle =     {Proceedings of the IWCS workshop on Foundations of
                   Situated and Multimodal Communication},
  editor =        {Asher, Nicholas and Hunter, Julie and
                   Lascarides, Alex},
  pages =         {1--8},
  title =         {Exploring multi-modal text+image models to
                   distinguish between abstract and concrete nouns},
  year =          {2017},
  url =           {http://www.aclweb.org/anthology/W17-7101},
}

@book{Bird:2009ab,
  address =       {Beijing, Cambridge, Farnham, K{\"o}ln, Sebastopol and
                   Tokyo},
  author =        {Bird, Steven and Klein, Ewan and Loper, Edward},
  edition =       {1st ed},
  publisher =     {O'Reilly},
  title =         {Natural language processing with Python},
  year =          {2009},
  abstract =      {This is an introduction to natural language
                   processing, which supports a variety of language
                   technologies, from predictive text and email
                   filtering to automatic summarization and translation},
  isbn =          {9780596516499 (pbk.)},
}

@book{BlackburnBos:2005,
  author =        {Blackburn, Patrick and Bos, Johan},
  publisher =     {CSLI Publications},
  title =         {Representation and inference for natural language.
                   {A} first course in computational semantics},
  year =          {2005},
  url =           {http://www.let.rug.nl/bos/comsem/book1.html},
}

@inproceedings{Bruni:2012kx,
  address =       {Jeju Island, Korea},
  author =        {Bruni, Elia and Boleda, Gemma and Baroni, Marco and
                   Tran, Nam Khanh},
  booktitle =     {Proceedings of the 50th Annual Meeting of the
                   Association for Computational Linguistics (Volume 1:
                   Long Papers)},
  month =         {July},
  pages =         {136--145},
  publisher =     {Association for Computational Linguistics},
  title =         {Distributional Semantics in Technicolor},
  year =          {2012},
  url =           {http://www.aclweb.org/anthology/P12-1015},
}

@incollection{Clark:2015aa,
  author =        {Clark, Stephen},
  booktitle =     {Handbook of Contemporary Semantics --- second
                   edition},
  chapter =       {16},
  editor =        {Lappin, Shalom and Fox, Chris},
  pages =         {493--522},
  publisher =     {Wiley -- Blackwell},
  title =         {Vector Space Models of Lexical Meaning},
  year =          {2015},
}

@techreport{Clark:2016aa,
  author =        {Stephen Clark and Laura Rimell and Tamara Polajnar and
                   Jean Maillard},
  institution =   {University of Cambridge Computer Laboratory},
  title =         {The Categorial Framework for Compositional
                   Distributional Semantics},
  year =          {2016},
}

@article{Connell:2014aa,
  author =        {Connell, Louise and Lynott, Dermot},
  journal =       {Topics in Cognitive Science},
  number =        {3},
  pages =         {390-406},
  title =         {Principles of Representation: Why You Can't Represent
                   the Same Concept Twice},
  volume =        {6},
  year =          {2014},
  abstract =      {Abstract As embodied theories of cognition are
                   increasingly formalized and tested, care must be
                   taken to make informed assumptions regarding the
                   nature of concepts and representations. In this
                   study, we outline three reasons why one cannot, in
                   effect, represent the same concept twice. First,
                   online perception affects offline representation:
                   Current representational content depends on how
                   ongoing demands direct attention to modality-specific
                   systems. Second, language is a fundamental
                   facilitator of offline representation: Bootstrapping
                   and shortcuts within the computationally cheaper
                   linguistic system continuously modify
                   representational content. Third, time itself is a
                   source of representational change: As the content of
                   underlying concepts shifts with the accumulation of
                   direct and vicarious experience, so too does the
                   content of representations that draw upon these
                   concepts. We discuss the ramifications of these
                   principles for research into both human and synthetic
                   cognitive systems.},
  doi =           {10.1111/tops.12097},
  url =           {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12097},
}

@article{Cooper:2015aa,
  author =        {Cooper, Robin and Dobnik, Simon and Lappin, Shalom and
                   Larsson, Staffan},
  journal =       {Linguistic Issues in Language Technology --- {LiLT}},
  month =         {November},
  number =        {4},
  pages =         {1--43},
  title =         {Probabilistic Type Theory and Natural Language
                   Semantics},
  volume =        {10},
  year =          {2015},
  url =           {https://gup.ub.gu.se/publication/226922},
}

@unpublished{Cooper:2016ac,
  author =        {Cooper, Robin},
  month =         {November 30},
  note =          {Draft of chapters 1-6},
  title =         {Type theory and language: From perception to
                   linguistic communication},
  year =          {2016},
  url =           {https://sites.google.com/site/typetheorywithrecords/drafts/
                  ttl150611.pdf},
}

@book{Eijck:2010aa,
  address =       {Cambridge},
  author =        {Eijck, J. van and Unger, Christina},
  publisher =     {Cambridge University Press},
  title =         {Computational semantics with functional programming},
  year =          {2010},
  isbn =          {9780521760300},
}

@article{Erk:2012aa,
  author =        {Erk, Katrin},
  journal =       {Language and Linguistics Compass},
  number =        {10},
  pages =         {635--653},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Vector Space Models of Word Meaning and Phrase
                   Meaning: A Survey},
  volume =        {6},
  year =          {2012},
  abstract =      {Distributional models represent a word through the
                   contexts in which it has been observed. They can be
                   used to predict similarity in meaning, based on the
                   distributional hypothesis, which states that two
                   words that occur in similar contexts tend to have
                   similar meanings. Distributional approaches are often
                   implemented in vector space models. They represent a
                   word as a point in high-dimensional space, where each
                   dimension stands for a context item, and a word's
                   coordinates represent its context counts. Occurrence
                   in similar contexts then means proximity in space. In
                   this survey we look at the use of vector space models
                   to describe the meaning of words and phrases: the
                   phenomena that vector space models address, and the
                   techniques that they use to do so. Many word meaning
                   phenomena can be described in terms of semantic
                   similarity: synonymy, priming, categorization, and
                   the typicality of a predicate's arguments. But vector
                   space models can do more than just predict semantic
                   similarity. They are a very flexible tool, because
                   they can make use of all of linear algebra, with all
                   its data structures and operations. The dimensions of
                   a vector space can stand for many things: context
                   words, or non-linguistic context like images, or
                   properties of a concept. And vector space models can
                   use matrices or higher-order arrays instead of
                   vectors for representing more complex relationships.
                   Polysemy is a tough problem for distributional
                   approaches, as a representation that is learned from
                   all of a word's contexts will conflate the different
                   senses of the word. It can be addressed, using either
                   clustering or vector combination techniques. Finally,
                   we look at vector space models for phrases, which are
                   usually constructed by combining word vectors. Vector
                   space models for phrases can predict phrase
                   similarity, and some argue that they can form the
                   basis for a general-purpose representation framework
                   for natural language semantics.},
  doi =           {10.1002/lnco.362},
  issn =          {1749-818X},
  url =           {http://dx.doi.org/10.1002/lnco.362},
}

@unpublished{Erk:2016aa,
  author =        {Katrin Erk},
  month =         {March},
  note =          {Manuscript},
  title =         {What do you know about an alligator when you know the
                   company it keeps?},
  year =          {2016},
}

@incollection{Grefenstette:2014aa,
  address =       {Dordrecht},
  author =        {Grefenstette, Edward and Sadrzadeh, Mehrnoosh and
                   Clark, Stephen and Coecke, Bob and Pulman, Stephen},
  booktitle =     {Computing Meaning},
  editor =        {Bunt, Harry and Bos, Johan and Pulman, Stephen},
  pages =         {71--86},
  publisher =     {Springer Netherlands},
  title =         {Concrete Sentence Spaces for Compositional
                   Distributional Models of Meaning},
  volume =        {4},
  year =          {2014},
  doi =           {10.1007/978-94-007-7284-7_5},
  isbn =          {978-94-007-7284-7},
  url =           {http://dx.doi.org/10.1007/978-94-007-7284-7_5},
}

@article{Herbelot:2017aa,
  author =        {Aur{\'{e}}lie Herbelot and Marco Baroni},
  journal =       {arXiv},
  pages =         {1--6},
  title =         {High-risk learning: acquiring new word vectors from
                   tiny data},
  volume =        {arXiv:1707.06556 [cs.CL]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1707.06556},
}

@inproceedings{Matuszek:2012aa,
  address =       {Edinburgh, Scotland},
  author =        {Matuszek, Cynthia and FitzGerald, Nicholas and
                   Zettlemoyer, Luke and Bo, Liefeng and Fox, Dieter},
  booktitle =     {Proceedings of the 29th International Conference on
                   Machine Learning (ICML 2012)},
  editor =        {John Langford and Joelle Pineau},
  month =         {June 27th - July 3rd},
  title =         {A joint model of language and perception for grounded
                   attribute learning},
  year =          {2012},
}

@inproceedings{Matuszek:2012uq,
  author =        {Cynthia Matuszek and Evan Herbst and Luke Zettlemoyer and
                   Dieter Fox},
  booktitle =     {Proceedings of the 13th International Symposium on
                   Experimental Robotics (ISER)},
  month =         {June},
  title =         {Learning to Parse Natural Language Commands to a
                   Robot Control System},
  year =          {2012},
}

@article{Monroe:2017ab,
  author =        {Monroe, Will and Hawkins, Robert X. D. and
                   Goodman, Noah D. and Potts, Christopher},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {325--338},
  title =         {Colors in Context: A Pragmatic Neural Model for
                   Grounded Language Understanding},
  volume =        {5},
  year =          {2017},
  abstract =      {We present a model of pragmatic referring expression
                   interpretation in a grounded communication task
                   (identifying colors from descriptions) that draws
                   upon predictions from two recurrent neural network
                   classifiers, a speaker and a listener, unified by a
                   recursive pragmatic reasoning framework. Experiments
                   show that this combined pragmatic model interprets
                   color descriptions more accurately than the
                   classifiers from which it is built, and that much of
                   this improvement results from combining the speaker
                   and listener perspectives. We observe that pragmatic
                   reasoning helps primarily in the hardest cases: when
                   the model must distinguish very similar colors, or
                   when few utterances adequately express the target
                   color. Our findings make use of a newly-collected
                   corpus of human utterances in color reference games,
                   which exhibit a variety of pragmatic behaviors. We
                   also show that the embedded speaker model reproduces
                   many of these pragmatic behaviors.},
  issn =          {2307-387X},
}

@article{Roy:2002uq,
  author =        {Deb Roy and Alex P. Pentland},
  journal =       {Cognitive Science},
  number =        {1},
  pages =         {113--146},
  title =         {Learning words from sights and sounds: a
                   computational model},
  volume =        {26},
  year =          {2002},
  doi =           {http://dx.doi.org/10.1016/S0364-0213(01)00061-1},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S0364021301000611},
}

@article{Roy:2005,
  address =       {Essex, UK},
  author =        {Roy, Deb},
  journal =       {Artificial Intelligence},
  month =         {September},
  number =        {1-2},
  pages =         {170--205},
  publisher =     {Elsevier Science Publishers Ltd.},
  title =         {Semiotic schemas: a framework for grounding language
                   in action and perception},
  volume =        {167},
  year =          {2005},
  doi =           {10.1016/j.artint.2005.04.007},
  issn =          {0004-3702},
}

@article{Socher:2014aa,
  author =        {Socher, Richard and Karpathy, Andrej and Le, Quoc V and
                   Manning, Christopher D and Ng, Andrew Y},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {207--218},
  title =         {Grounded compositional semantics for finding and
                   describing images with sentences},
  volume =        {2},
  year =          {2014},
}

@article{Turney:2010aa,
  author =        {Turney, Peter D and Pantel, Patrick and others},
  journal =       {Journal of artificial intelligence research},
  number =        {1},
  pages =         {141--188},
  title =         {From frequency to meaning: Vector space models of
                   semantics},
  volume =        {37},
  year =          {2010},
  doi =           {http://dx.doi.org/10.1613/jair.2934},
}

@inproceedings{Vulic:2018aa,
  author =        {Vuli{\'{c}}, Ivan and Mrk{\v{s}}i{\'{c}}, Nikola},
  booktitle =     {Proceedings of the 2018 Conference of the North
                   American Chapter of the Association for Computational
                   Linguistics: Human Language Technologies, Volume 1
                   (Long Papers)},
  pages =         {1134--1145},
  publisher =     {Association for Computational Linguistics},
  title =         {Specialising Word Vectors for Lexical Entailment},
  year =          {2018},
  url =           {http://aclweb.org/anthology/N18-1103},
}

